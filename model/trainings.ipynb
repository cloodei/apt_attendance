{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "7evg7p3dh3RN",
        "wusRSbvTh6Z-",
        "Sn24dDWfiB1Z",
        "vbbjwOIbgTrz",
        "q_OKjxiTfjP6",
        "3-mbyVJZKXhf",
        "dlh4gUNGKdU-",
        "tryj7Bvqdn3h",
        "z3gHfQlIeOhc"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cloodei/apt_attendance/blob/main/model/trainings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLCIYlzONv4w"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "input_folder = '/content/drive/MyDrive/ai_techwiz_2/data'\n",
        "cropped_folder = '/content/drive/MyDrive/ai_techwiz_2/face_dataset_cropped'\n",
        "embeddings_folder = '/content/drive/MyDrive/ai_techwiz_2/face_embeddings'\n",
        "\n",
        "os.makedirs(input_folder, exist_ok=True)\n",
        "os.makedirs(cropped_folder, exist_ok=True)\n",
        "os.makedirs(embeddings_folder, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install facenet-pytorch\n"
      ],
      "metadata": {
        "id": "FmxrbAgfOGYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hnswlib\n"
      ],
      "metadata": {
        "id": "Zhshc-a6HFoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n"
      ],
      "metadata": {
        "id": "0FxLAl2c4G3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image, ImageFilter, ImageOps\n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1, extract_face, fixed_image_standardization\n",
        "from tqdm import tqdm\n",
        "import joblib\n",
        "import cv2\n"
      ],
      "metadata": {
        "id": "t3EAnj4QOG5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data preprocessing**"
      ],
      "metadata": {
        "id": "7evg7p3dh3RN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class FaceRecognitionConfig:\n",
        "    DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    IMAGE_SIZE = 160\n",
        "    MARGIN = 20\n",
        "    MIN_FACE_SIZE = 20\n",
        "    THRESHOLDS = [0.6, 0.7, 0.7]\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "def ensure_dir(path: str):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "BFeTLLM9OWbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_mtcnn(device=FaceRecognitionConfig.DEVICE, image_size=160, margin=20) -> MTCNN:\n",
        "    return MTCNN(\n",
        "        image_size=image_size,\n",
        "        margin=margin,\n",
        "        min_face_size=FaceRecognitionConfig.MIN_FACE_SIZE,\n",
        "        thresholds=FaceRecognitionConfig.THRESHOLDS,\n",
        "        factor=0.709,\n",
        "        post_process=False,\n",
        "        device=device,\n",
        "        keep_all=False,\n",
        "        select_largest=True\n",
        "    )\n",
        "\n",
        "def get_resnet(device=FaceRecognitionConfig.DEVICE, pretrained='vggface2') -> InceptionResnetV1:\n",
        "    return InceptionResnetV1(pretrained=pretrained).eval().to(device)\n",
        "\n",
        "mtcnn = get_mtcnn()\n",
        "resnet = get_resnet()\n",
        "print(\"Models ready. Device:\", FaceRecognitionConfig.DEVICE)\n"
      ],
      "metadata": {
        "id": "S3cgFKG4OkAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Face Detection**"
      ],
      "metadata": {
        "id": "wusRSbvTh6Z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def process_dataset(input_root: str, output_root: str, mtcnn: MTCNN = None):\n",
        "\n",
        "    if mtcnn is None:\n",
        "        mtcnn = get_mtcnn()\n",
        "\n",
        "    ensure_dir(output_root)\n",
        "    persons = [d for d in os.listdir(input_root)\n",
        "               if os.path.isdir(os.path.join(input_root, d)) and not d.startswith('.')]\n",
        "\n",
        "    for person in persons:\n",
        "        person_input_dir = os.path.join(input_root, person)\n",
        "        person_output_dir = os.path.join(output_root, person)\n",
        "        ensure_dir(person_output_dir)\n",
        "\n",
        "        img_list = [f for f in os.listdir(person_input_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "        for img_name in tqdm(img_list, desc=f\"Processing {person}\", ncols=100):\n",
        "            img_path = os.path.join(person_input_dir, img_name)\n",
        "            pil_img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "            face, prob = mtcnn(pil_img, return_prob=True)\n",
        "            if face is None:\n",
        "                print(f\"Không tìm thấy mặt trong ảnh {img_path}\")\n",
        "                continue\n",
        "            ensure_dir(person_output_dir)\n",
        "\n",
        "            tensor_path = os.path.join(person_output_dir, img_name.replace(\".png\", \".pt\"))\n",
        "            torch.save(face, tensor_path)\n",
        "\n",
        "    print(\"Xử lý dataset xong (lưu tensor)!\")\n",
        "\n",
        "\n",
        "process_dataset(input_folder, cropped_folder, mtcnn)"
      ],
      "metadata": {
        "id": "vrKt6HExQWVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Face Embedding**"
      ],
      "metadata": {
        "id": "Sn24dDWfiB1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NAME_TO_MSV = {\n",
        "    \"Nguyen_Van_Tuan\": \"SV_0007\",\n",
        "    \"Nguyen_Viet_Quoc_An\": \"SV_0008\",\n",
        "    \"Nguyen_Van_Minh\": \"SV_0006\",\n",
        "    \"Nguyen_Thi_Phuong_Thao\": \"SV_0013\",\n",
        "    \"Nguyen_Thi_Hong_Mai\": \"SV_0011\",\n",
        "    \"Nguyen_Thi_Cam_Ly\": \"SV_0010\",\n",
        "    \"Nguyen_The_Truong\": \"SV_0009\",\n",
        "    \"Nguyen_Phu_Nguyen\": \"SV_0005\",\n",
        "    \"Nguyen_Phong_Hai\": \"SV_0004\",\n",
        "    \"Nguyen_Ha_Phuong_Uyen\": \"SV_0012\",\n",
        "    \"Nguyen_Duy_Hoang\": \"SV_0001\",\n",
        "    \"Nguyen_Duc_Phong\": \"SV_0002\",\n",
        "    \"Mai_Thanh_Thu\": \"SV_0014\",\n",
        "    \"Le_Duc_Nguyen\": \"SV_0003\"\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "id": "5zpNTukggFDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "def pil_to_tensor_standard(pil_img: Image.Image) -> torch.Tensor:\n",
        "\n",
        "    arr = np.asarray(pil_img).astype(np.float32) / 255.0\n",
        "    t = torch.from_numpy(arr.transpose(2, 0, 1)).float()\n",
        "    return fixed_image_standardization(t)\n",
        "\n",
        "\n",
        "def compute_embeddings_all(\n",
        "    cropped_root: str,\n",
        "    embeddings_root: str,\n",
        "    resnet: InceptionResnetV1 = None,\n",
        "    batch_size: int = 32,\n",
        "    name_to_msv: dict = None\n",
        "):\n",
        "    if resnet is None:\n",
        "        resnet = get_resnet()\n",
        "\n",
        "    registry = {}\n",
        "\n",
        "    persons = [d for d in os.listdir(cropped_root) if os.path.isdir(os.path.join(cropped_root, d))]\n",
        "\n",
        "    for person in tqdm(persons, desc=\"Compute embeddings (all)\"):\n",
        "        person_dir = os.path.join(cropped_root, person)\n",
        "        tensor_files = [f for f in os.listdir(person_dir) if f.lower().endswith('.pt')]\n",
        "\n",
        "        if not tensor_files:\n",
        "            continue\n",
        "\n",
        "        face_tensors = []\n",
        "        for tf in tensor_files:\n",
        "            tensor_path = os.path.join(person_dir, tf)\n",
        "            face = torch.load(tensor_path)              # [3,H,W], float [0,1]\n",
        "            face = fixed_image_standardization(face)    # [-1,1]\n",
        "            face_tensors.append(face)\n",
        "\n",
        "        face_stack = torch.stack(face_tensors)  # [N,3,H,W]\n",
        "\n",
        "\n",
        "        all_embeddings = []\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(face_stack), batch_size):\n",
        "                batch = face_stack[i:i+batch_size]\n",
        "                emb = resnet(batch)  # [B,512]\n",
        "                all_embeddings.append(emb)\n",
        "        all_embeddings = torch.cat(all_embeddings, dim=0)  # [N,512]\n",
        "\n",
        "        student_id = name_to_msv.get(person, person) if name_to_msv else person\n",
        "\n",
        "        registry[student_id] = all_embeddings.cpu().numpy()\n",
        "\n",
        "    os.makedirs(embeddings_root, exist_ok=True)\n",
        "    np.savez(os.path.join(embeddings_root, \"embeddings_all.npz\"), **registry)\n",
        "\n",
        "\n",
        "compute_embeddings_all(cropped_folder, embeddings_folder, resnet, name_to_msv=NAME_TO_MSV)"
      ],
      "metadata": {
        "id": "bXgqPOlXf4uP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train/test split**"
      ],
      "metadata": {
        "id": "vbbjwOIbgTrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_data = '/content/drive/MyDrive/ai_techwiz_2/face_embeddings'\n",
        "data = np.load(os.path.join(embeddings_data, \"embeddings_all.npz\"))\n",
        "dict1 = {k: data[k] for k in data.files}"
      ],
      "metadata": {
        "id": "k1q-2-JIClWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict1.keys()"
      ],
      "metadata": {
        "id": "5FSc-_u6N80-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_dict = dict(sorted(dict1.items()))\n",
        "sorted_dict.keys()"
      ],
      "metadata": {
        "id": "NmpLPcxBOLBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = sorted_dict\n",
        "X, y = [], []\n",
        "\n",
        "label_map, reverse_map = {}, {}\n",
        "\n",
        "label_counter = 1\n",
        "\n",
        "for student_id, vectors in data.items():\n",
        "    if student_id not in label_map:\n",
        "        label_map[student_id] = label_counter\n",
        "        reverse_map[label_counter] = student_id\n",
        "        label_counter += 1\n",
        "    for v in vectors:\n",
        "        X.append(v)\n",
        "        y.append(label_map[student_id])\n",
        "\n",
        "X = np.array(X).astype('float32')\n",
        "y = np.array(y).astype('int')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "9TePnR2qgaxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cosine**"
      ],
      "metadata": {
        "id": "q_OKjxiTfjP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = []\n",
        "for x in X_test:\n",
        "    sims = cosine_similarity([x], X_train)[0]\n",
        "    idx_max = np.argmax(sims)\n",
        "    y_pred.append(y_train[idx_max])\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", acc)"
      ],
      "metadata": {
        "id": "AY12nejU_cG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model: SVM**"
      ],
      "metadata": {
        "id": "3-mbyVJZKXhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "svm = SVC(kernel=\"linear\", probability=True, random_state=42)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "y_pred = svm.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", acc)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "N9pV-MGJOsEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Model: HNSW**"
      ],
      "metadata": {
        "id": "dlh4gUNGKdU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hnswlib\n",
        "\n",
        "dim = X.shape[1]\n",
        "num_elements = X_train.shape[0]\n",
        "\n",
        "index = hnswlib.Index(space='cosine', dim=dim)\n",
        "index.init_index(max_elements=200000, ef_construction=200, M=32)\n",
        "index.add_items(X_train, y_train)\n",
        "index.set_ef(200)\n",
        "\n",
        "labels, distances = index.knn_query(X_test, k=1)\n",
        "\n",
        "y_pred = []\n",
        "for label, dist in zip(labels[:,0], distances[:,0]):\n",
        "    if dist > 0.3:\n",
        "        y_pred.append(0)\n",
        "    else:\n",
        "        y_pred.append(label)\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", acc)"
      ],
      "metadata": {
        "id": "knOBDJL9GmVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model: FAISS**"
      ],
      "metadata": {
        "id": "tryj7Bvqdn3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "\n",
        "dim = X_train.shape[1]\n",
        "\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "index.add(X_train)\n",
        "\n",
        "def predict_one_faiss_k1(index, x):\n",
        "    x = x.reshape(1, -1).astype('float32')\n",
        "    D, I = index.search(x, 1)\n",
        "    idx = I[0][0]\n",
        "    dist = D[0][0]\n",
        "    label = y_train[idx]\n",
        "\n",
        "    similarity = 1 / (1 + dist)\n",
        "    return label, similarity\n",
        "\n",
        "y_pred = []\n",
        "for x in X_test:\n",
        "    lbl, _ = predict_one_faiss_k1(index, x)\n",
        "    y_pred.append(lbl)\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy FAISS Flat:\", acc)\n"
      ],
      "metadata": {
        "id": "IkSieIQ34Czq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download model"
      ],
      "metadata": {
        "id": "z3gHfQlIeOhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "faiss_path = \"/content/drive/MyDrive/ai_techwiz_2/faiss_index.index\"\n",
        "labels_path = \"/content/drive/MyDrive/ai_techwiz_2/faiss_labels.pkl\"\n",
        "\n",
        "faiss.write_index(index, faiss_path)\n",
        "\n",
        "with open(labels_path, \"wb\") as f:\n",
        "    pickle.dump(y_train, f)"
      ],
      "metadata": {
        "id": "iqZ8VGm45NgK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}